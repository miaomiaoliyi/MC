{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import logging\n",
    "import re\n",
    "import json\n",
    "import jieba\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger('les2')\n",
    "logger.setLevel(logging.INFO)\n",
    "console_handler = logging.StreamHandler()\n",
    "# console_handler.setLevel(logging.DEBUG)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "console_handler.setFormatter(formatter)\n",
    "logger.addHandler(console_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./data/question.json', 'r', encoding='utf-8') as f:\n",
    "    train_set = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 训练集共20000篇文章\n",
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def precision_recall_f1(prediction, ground_truth):\n",
    "    if not isinstance(prediction, list):\n",
    "        prediction_tokens = prediction.split()\n",
    "    else:\n",
    "        prediction_tokens = prediction\n",
    "    if not isinstance(ground_truth, list):\n",
    "        ground_truth_tokens = ground_truth.split()\n",
    "    else:\n",
    "        ground_truth_tokens = ground_truth\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0, 0, 0\n",
    "    p = 1.0 * num_same / len(prediction_tokens)\n",
    "    r = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * p * r) / (p + r)\n",
    "    return p, r, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recall(prediction, ground_truth):\n",
    "    return precision_recall_f1(prediction, ground_truth)[1]\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    return precision_recall_f1(prediction, ground_truth)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def metric_max_over_ground_truths(metric_fn, prediction, ground_truth):\n",
    "    score = metric_fn(prediction, ground_truth)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 找到最相关的段落和在段落中的位置\n",
    "def find_fake_answer(sample):\n",
    "    for a_idx, answer_token in enumerate(sample['questions']):\n",
    "        most_related_para = -1\n",
    "        most_related_para_len = 999999\n",
    "        max_related_score = 0\n",
    "#         print('a_idx=',a_idx, 'answer_token=',answer_token)\n",
    "        for p_idx, para_tokens in enumerate(sample['segmented_article_content']):\n",
    "            related_score = metric_max_over_ground_truths(recall,\n",
    "                                                          para_tokens,\n",
    "                                                          answer_token['segmented_answer'])\n",
    "#             print('p_idx=',p_idx,'related_score=',related_score)\n",
    "            if related_score > max_related_score \\\n",
    "                    or (related_score == max_related_score\n",
    "                        and len(para_tokens) < most_related_para_len):\n",
    "                most_related_para = p_idx\n",
    "                most_related_para_len = len(para_tokens)\n",
    "                max_related_score = related_score\n",
    "        sample['questions'][a_idx]['most_related_para'] = most_related_para\n",
    "        most_related_para_tokens = sample['segmented_article_content'][most_related_para]\n",
    "        \n",
    "        answer_tokens = set(answer_token['segmented_answer'])\n",
    "        best_match_score = 0\n",
    "        best_match_span = [-1, -1]\n",
    "        best_fake_answer = None\n",
    "        \n",
    "        for start_tidx in range(len(most_related_para_tokens)):\n",
    "            if most_related_para_tokens[start_tidx] not in answer_tokens:\n",
    "                continue\n",
    "            for end_tidx in range(len(most_related_para_tokens) - 1, start_tidx - 1, -1):\n",
    "                span_tokens = most_related_para_tokens[start_tidx: end_tidx + 1]\n",
    "                match_score = metric_max_over_ground_truths(f1_score, span_tokens,\n",
    "                                                                answer_token['segmented_answer'])\n",
    "                if match_score == 0:\n",
    "                    break\n",
    "                if match_score > best_match_score:\n",
    "                    best_match_span = [start_tidx, end_tidx]\n",
    "                    best_match_score = match_score\n",
    "                    best_fake_answer = ''.join(span_tokens)\n",
    "        sample['questions'][a_idx]['answer_spans'] = best_match_span\n",
    "        sample['questions'][a_idx]['fake_answers'] = best_fake_answer\n",
    "        sample['questions'][a_idx]['match_scores'] = best_match_score\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_data(sample):\n",
    "    # 文章内容和标题分段->分词：将标题插入到分段后的首位置\n",
    "    sample['segmented_article_title'] = \\\n",
    "        list(jieba.cut(''.join(re.split(r'\\u3000+|\\s+|\\t+',sample['article_title'].strip()))))\n",
    "    \n",
    "    sample_splited_para = re.split(r'\\u3000+|\\s+|\\t+',sample['article_content'].strip())\n",
    "    if len(sample_splited_para) == 1 and len(sample_splited_para[0]) > 200:\n",
    "        sample_splited_para = re.split(r'\\。',sample['article_content'].strip())\n",
    "    sample_splited_list = []\n",
    "    for para in sample_splited_para:\n",
    "        sample_splited_list.append(list(jieba.cut(para.strip(), cut_all=False)))\n",
    "    sample_splited_list.insert(0, sample['segmented_article_title'])\n",
    "\n",
    "    sample['segmented_article_content'] = sample_splited_list\n",
    "       \n",
    "    # 问题和答案分词处理\n",
    "    for i,question in enumerate(sample['questions']):\n",
    "        sample['questions'][i]['segmented_question'] = \\\n",
    "            list(jieba.cut(''.join(question['question'].strip().split('\\u3000+|\\s+|\\t+'))))\n",
    "        sample['questions'][i]['segmented_answer'] = \\\n",
    "            list(jieba.cut(''.join(question['answer'].strip().split('\\u3000+|\\s+|\\t+'))))\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def store_prerpocess_data():\n",
    "    preprocess_data = []\n",
    "    for i in range(1,201):\n",
    "        with open('./data/preprocessed_%d.json' % i, 'r', encoding='utf-8') as f:\n",
    "            d = json.load(f)\n",
    "        preprocess_data.extend(d)\n",
    "    with open('./data/preprocessed.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(preprocess_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 数据去重\n",
    "with open('./data/preprocessed.json', 'r', encoding='utf-8') as f:\n",
    "    data_preprocessed = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "title = Counter([data_preprocessed[i]['article_title'] for i in range(len(data_preprocessed))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "{x : title[x] for x in title if title[x] >= 2 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "title_set = set()\n",
    "data_qc = []\n",
    "for sample in data_preprocessed:\n",
    "    title = sample['article_title']\n",
    "    if title in title_set:\n",
    "        continue\n",
    "    else:\n",
    "        title_set.add(title)\n",
    "        data_qc.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(data_qc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_qc[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('./data/preprocessed_qc.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data_qc, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test_split(dataset,train_percent=0.9):\n",
    "    index = np.arange(len(dataset))\n",
    "    np.random.shuffle(index)\n",
    "\n",
    "    train_size = int(len(dataset) * train_percent)\n",
    "    train_index = index[:train_size]\n",
    "    test_index = index[train_size:]\n",
    "    train_set, test_set = [], []\n",
    "    for index in train_index:\n",
    "        train_set.append(dataset[index])\n",
    "    for index in test_index:\n",
    "        test_set.append(dataset[index])\n",
    "        \n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainset, testset = train_test_split(data_qc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LESDataset(object):\n",
    "    def __init__(self, max_p_len, max_q_len,vocab, train_file=None, test_file=None):\n",
    "        self.max_p_len = max_p_len\n",
    "        self.max_q_len = max_q_len\n",
    "        self.vocab = vocab\n",
    "        if train_file:\n",
    "            self.train_set = self._load_dataset(train_file)\n",
    "        if test_file:\n",
    "            self.test_set = self._load_dataset(test_file)\n",
    "\n",
    "    def _load_dataset(self, data_path, train=True):\n",
    "        \"\"\"\n",
    "        加载数据集\n",
    "        :param data_path:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        with open(data_path, 'r', encoding='utf-8') as f:\n",
    "            data_set = json.load(f)\n",
    "        if train:\n",
    "            data = []\n",
    "            for sample in data_set:\n",
    "                for qa_pairs in sample['questions']:\n",
    "                    if qa_pairs['answer_spans'][0] == -1:\n",
    "                        continue\n",
    "                    data.append({'question':qa_pairs['segmented_question'],\n",
    "                                'passage':sample['segmented_article_content'][qa_pairs['most_related_para']],\n",
    "                                'answer_span':qa_pairs['answer_spans']})\n",
    "        return data\n",
    "    \n",
    "    def word_iter(self, set_name):\n",
    "        if set_name == 'train':\n",
    "            data_set = self.train_set\n",
    "\n",
    "        for sample in data_set:\n",
    "            for question in sample['questions']:\n",
    "                for word in question['segmented_question']:\n",
    "                    yield word\n",
    "                for word in sample['segmented_article_content'][question['most_related_para']]:\n",
    "                    yield word\n",
    "                    \n",
    "    def gen_mini_batches(self, set_name, batch_size, pad_id=0,shuffle=True):\n",
    "        if set_name == 'train':\n",
    "            data = self.train_set\n",
    "            \n",
    "        data_size = len(data)\n",
    "        indices = np.arange(data_size)\n",
    "        if shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "        for batch_start in np.arange(0, data_size, batch_size):\n",
    "            batch_indices = indices[batch_start:batch_start+batch_size]\n",
    "            batch_data = [data[i] for i in batch_indices]\n",
    "            yield self._one_mini_batch(batch_data, pad_id)\n",
    "            \n",
    "    def _one_mini_batch(self, batch_data_raw, pad_id):\n",
    "        batch_data = {'question_token_ids':[],\n",
    "                     'question_length':[],\n",
    "                     'passage_token_ids':[],\n",
    "                     'passage_length':[],\n",
    "                     'start_id':[],\n",
    "                     'end_id':[]}\n",
    "        for qa_pairs in batch_data_raw:\n",
    "            batch_data['question_token_ids'].append(self.convert_to_ids(qa_pairs['question'])),\n",
    "            batch_data['question_length'].append(len(qa_pairs['question']))\n",
    "            batch_data['passage_token_ids'].append(self.convert_to_ids(qa_pairs['passage']))\n",
    "            batch_data['passage_length'].append(len(qa_pairs['passage']))\n",
    "            batch_data['start_id'].append(qa_pairs['answer_span'][0])\n",
    "            batch_data['end_id'].append(qa_pairs['answer_span'][1])\n",
    "            \n",
    "        batch_data = self._dynamic_padding(batch_data, pad_id)\n",
    "        return batch_data\n",
    "    \n",
    "    def _dynamic_padding(self, batch_data, pad_id):\n",
    "        pad_p_len = min(self.max_p_len, max(batch_data['passage_length']))\n",
    "        pad_q_len = min(self.max_q_len, max(batch_data['question_length']))\n",
    "        batch_data['passage_token_ids'] = [(ids + [pad_id] * (pad_p_len - len(ids)))[:pad_p_len]\n",
    "                                                for ids in batch_data['passage_token_ids']]\n",
    "        batch_data['question_token_ids'] = [(ids + [pad_id] * (pad_q_len - len(ids)))[:pad_q_len]\n",
    "                                                for ids in batch_data['question_token_ids']]\n",
    "        return batch_data\n",
    "            \n",
    "        \n",
    "    def convert_to_ids(self,tokens):\n",
    "        ids = []\n",
    "        for token in tokens:\n",
    "            ids.append(self.vocab.token2id[token.lower()])\n",
    "        return ids\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "\n",
    "    def __init__(self, filename=None, lower=False):\n",
    "        self.id2token = {}\n",
    "        self.token2id = {}\n",
    "        self.token_cnt = defaultdict(int)\n",
    "        self.lower = lower\n",
    "\n",
    "        self.embed_dim = None\n",
    "        self.embeddings = None\n",
    "\n",
    "        self.pad_token = '<blank>'\n",
    "        self.unk_token = '<unk>'\n",
    "\n",
    "        self.initial_tokens = []\n",
    "        self.initial_tokens.extend([self.pad_token, self.unk_token])\n",
    "\n",
    "        for token in self.initial_tokens:\n",
    "            self.add(token)\n",
    "\n",
    "\n",
    "    def add(self, token, cnt=True):\n",
    "        token = token.lower() if self.lower else token\n",
    "\n",
    "        if token in self.token2id:\n",
    "            idx = self.token2id[token]\n",
    "        else:\n",
    "            idx = len(self.token2id)\n",
    "            self.token2id[token] = idx\n",
    "            self.id2token[idx] = token\n",
    "        if cnt:\n",
    "            self.token_cnt[token] += 1\n",
    "\n",
    "        return idx\n",
    "    \n",
    "    def randomly_init_embeddings(self, embed_dim):\n",
    "        self.embed_dim = embed_dim\n",
    "        self.embeddings = np.random.rand(len(self.token2id), embed_dim)\n",
    "        \n",
    "        for token in [self.pad_token, self.unk_token]:\n",
    "            self.embeddings[self.token2id[token]] = np.zeros([embed_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "les_dataset = LESDataset(300, 60,vocab ,train_file='F:\\\\jupyter_file\\\\MC\\\\data\\\\trainset.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batches = les_dataset.gen_mini_batches('train', 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch['question_token_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aa = [1,2,3,4,5,6,7,89,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = [1,3,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6730c6850652>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maa\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not list"
     ]
    }
   ],
   "source": [
    "aa[b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = tf.Variable(3, 'k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = tf.Variable(2, 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder('int32', [None],name='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = tf.add(tf.multiply(k, x) , b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(y, feed_dict={x:[3]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Variable/read:0\", shape=(), dtype=int32)\n",
      "Tensor(\"Variable_1/read:0\", shape=(), dtype=int32)\n",
      "Tensor(\"Variable_2/read:0\", shape=(), dtype=int32)\n",
      "Tensor(\"Variable_3/read:0\", shape=(), dtype=int32)\n",
      "Tensor(\"Variable_4/read:0\", shape=(), dtype=int32)\n",
      "Tensor(\"Variable_5/read:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "vs = tf.trainable_variables()\n",
    "for v in vs:\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "v1 = tf.Variable([1.0,2.3], name='v1')\n",
    "v2 = tf.Variable(55.5, name='v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ckpt_path = './test.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.run(init_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_path = saver.save(sess, ckpt_path, global_step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "array = np.arange(25).reshape(5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3,  4],\n",
       "       [ 5,  6,  7,  8,  9],\n",
       "       [10, 11, 12, 13, 14],\n",
       "       [15, 16, 17, 18, 19],\n",
       "       [20, 21, 22, 23, 24]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = tf.nn.embedding_lookup(array, [[1, 3],[2,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 5  6  7  8  9]\n",
      "  [15 16 17 18 19]]\n",
      "\n",
      " [[10 11 12 13 14]\n",
      "  [15 16 17 18 19]]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'c' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-8520a84ac8bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'c' is not defined"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(b))\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
